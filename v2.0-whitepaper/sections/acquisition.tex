\section{Acquisition of data}
% Introduce and group ATLAS+CMS+ALICE/LHCb
The first stage of any trigger system involves the readout of detector information (e.g.,  energy deposits in a calorimeter/tracker) and processing of this information. This processing has two main facets: event-building, synchronising information from separate subdetectors to ensure that the corresponding data are correctly associated, and the reduction of data volume. The latter is generally accomplished through a combination of basic compression techniques (e.g., zero suppression) and simple selections. These simple selections are commonly performed before readout and event-building, meaning that the information is highly localised to the subdetectors with limited possibilities of combining the information. This is the common task of a low-level trigger. This section covers only ATLAS, CMS and ALICE, since LHCb no longer employs a low-level trigger: detector information is read out continuously by TELL40 readout boards (with compression applied upon readout) and passed to the event builder InfiniBand-based network of HLT1~\cite{LHCb:2023hlw}. The smaller event size and non-hermetic layout(readout cables sit outside detector acceptance) means this change is more achievable for LHCb than ATLAS and CMS.

%Large ATLAS on tiered trig sys, similar structure in CMS, slight difference in ALICE, then tiny LHCb caveat/sentence to be discussed later

%ATLAS+CMS+ALICE
The initial hardware-based L1 triggers of ATLAS~\cite{ATLASRun3Detector} and CMS~\cite{cms2023development} both reduce the rate of data down to a maximum detector read out limit of \SI{100}{\kilo\hertz}, within a latency of \SI{2.5}{\micro\second} at ATLAS and \SI{4}{\micro\second} at CMS. The systems consist of custom ASIC- and FPGA-based\footnote{Algorithm Specific Integrated Chips and Field Programmable Gate Arrays, respectively~\cite{asics-fpgas}.} electronics which use reduced granular information from the calorimeter and muon systems to perform coarse selections. Upon an event passing this selection, the L1 directs the readout hardware of each subdetector to process the surviving data, which is stored in associated local buffers. Information from each subdetector is  then readout, processed and combined within a central readout system where events are assembled. It is buffered in the HLT farm until requested for processing by the HLT. The data acquisition (DAQ) systems of both experiments have evolved to use consumer network and computing hardware downstream of custom on-board electronics. This setup simplifies the readout in complexity, maintenance cost and upgrade capabilities. At ATLAS the Front-End LInk eXchange (FELIX)~\cite{ATLAS:FELIX} readout boards, responsible for the interface between commercial and custom hardware, have been partially implemented in Run 3 and will be fully implemented for HL-LHC.


In Run~3, several of the ALICE subdetectors have been upgraded to read out data continuously. The CTS synchronises data, subdividing readout into HeartBeat (HB) frames of approximately 1 LHC orbit period ($\sim\SI{88.92}{\micro\second}$). An HB frame is only kept if all relevant readout units (up to 441 in the entire detector) can be read out. Each HB decision is transmitted asynchronously to the First Level Processor, instructing it on what data to keep in a given HB frame. Whilst many subdetectors (including the TPC) were upgraded to read out data continuously in Run~3, continuous readout is not possible for some subdetectors, e.g.,  the Transition Radiation Detector. Such subdetectors are operated on a triggered basis and are hence excluded from the HB decision calculation, instead using the existing RD12 TTC protocol developed for Run~2 operation. Triggered subdetectors thus operate independently, with triggered data combined with continuous readout data at a later stage~\cite{alice-trigger-run3}.

